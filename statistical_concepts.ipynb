{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18618b75-63e8-42da-b2a0-bc79ef3d0ebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install statsmodels\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest, ttest_ind\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from scipy.stats import norm, chisquare, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38374db9",
   "metadata": {},
   "source": [
    "# Important statictical concepts for A/B testing\n",
    "In this Notebook I have implemented and simulated on some important statistical concepts for experimentation. Hopefully this can help someone build intuition around those concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09015c1b-0bb8-4ae6-9811-29e9819baeea",
   "metadata": {},
   "source": [
    "# Risks in experiments\n",
    "When we run experiments, one important goal is to bound the risks of making the incorrect decision. For any statistical test we have the intended risks, i.e., the values at wich we are trying to bound our risks, and the actual risks that we take in practice. If we fulfill all assumptions of a test (often large enough sample) and we have designed our experiment properly, we can choose the intended risks before the experiment, and then obtain these risk levels in practice. However, if we do not meet the assumptions of the test or somehow missuse the test, we may get much larger risks than intended. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813edef0",
   "metadata": {},
   "source": [
    "## False positive rate \n",
    "The intended false positive rate, often denoted as alpha is the rate at which we will detect an effect when in fact there is no effect. If we meet the assumptions of the test we are using, we obtain the intended false positve rate (or lower) over repeated experiments.\n",
    "\n",
    "Below I run a simulation with large normal samples. Both the t-test and the z-test are valid and should return the intended false positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43460a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041\n",
      "0.041\n"
     ]
    }
   ],
   "source": [
    "pvaluesZ = []\n",
    "pvaluesT = []\n",
    "\n",
    "reps = 1000\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "n = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(reps):\n",
    "    y0 = np.random.normal(mean, std_dev, n)\n",
    "    y1 = np.random.normal(mean, std_dev, n)\n",
    "    \n",
    "    stat, pvalue = ztest(x1=y1, x2=y0, alternative='two-sided')\n",
    "    pvaluesZ.append(pvalue<=alpha)\n",
    "    \n",
    "    res = ttest_ind(x1=y1, x2=y0, alternative='two-sided')\n",
    "    pvaluesT.append(res[1]<=alpha)\n",
    "    \n",
    "print(np.mean(pvaluesZ))\n",
    "print(np.mean(pvaluesT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292257f-0043-4292-8cc4-290f2b7e244d",
   "metadata": {},
   "source": [
    "For this sample size the false postive rate is the same for t and z, both close to alpha, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1225d4a",
   "metadata": {},
   "source": [
    "### Multiple testing correction\n",
    "If we have more than 1 metric (or several treatment groups) we have more than 1 chance to get a false positive result. A multiple testing correction counters this false positive rate inflation. The most common method is Bonferroni where we simply divide the intended false postive rate alpha by the number of metrics (or more generally the number of tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb08161-7afb-4311-b957-f07a7227064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate without mutliple testing correction: 0.164\n",
      "False positive rate with Bonferroni correction: 0.056\n"
     ]
    }
   ],
   "source": [
    "sig_no_bon = []\n",
    "sig_bon = []\n",
    "\n",
    "reps = 1000\n",
    "n_samples = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(reps):\n",
    "    df_experiment = pd.DataFrame({\n",
    "    'Metric1': np.random.normal(1, 2, n_samples*2),\n",
    "    'Metric2': np.random.normal(2, 2, n_samples*2),\n",
    "    'Metric3': np.random.normal(10, 2, n_samples*2),\n",
    "    'group': np.random.binomial(1,0.5,n_samples*2)\n",
    "    })\n",
    "\n",
    "    stat_1, pvalue_1 = ztest(x1=df_experiment[df_experiment['group']==0][\"Metric1\"], x2=df_experiment[df_experiment['group']==1][\"Metric1\"], alternative='two-sided')\n",
    "    stat_2, pvalue_2 = ztest(x1=df_experiment[df_experiment['group']==0][\"Metric2\"], x2=df_experiment[df_experiment['group']==1][\"Metric2\"], alternative='two-sided')\n",
    "    stat_3, pvalue_3 = ztest(x1=df_experiment[df_experiment['group']==0][\"Metric3\"], x2=df_experiment[df_experiment['group']==1][\"Metric3\"], alternative='two-sided')\n",
    "\n",
    "    alpha_bon_adjust = alpha/3\n",
    "    sig_no_bon.append(any([pvalue_1<=alpha, pvalue_2<=alpha, pvalue_3<=alpha]))\n",
    "    sig_bon.append(any([pvalue_1<=alpha_bon_adjust, pvalue_2<=alpha_bon_adjust, pvalue_3<=alpha_bon_adjust]))\n",
    "        \n",
    "print(f'False positive rate without mutliple testing correction: {np.mean(sig_no_bon)}')\n",
    "print(f'False positive rate with Bonferroni correction: {np.mean(sig_bon)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4e2cd-1486-4ed4-b54a-8e34fe80a1c9",
   "metadata": {},
   "source": [
    "As we can see, with 3 independent metrics the false positive rate is inflated a lot for finding at least one significant metric. The Bonferroni correction bounds the false positive rate below alpha as intended. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128916b",
   "metadata": {},
   "source": [
    "### Repeated testing (peeking)\n",
    "If we are looking at the statistical results during the data collection (for example to abort early if harm is detected) we have repeated chances to find a false positive result. We can correct for the false positive rate inflation by using so called sequential test. In principle, sequential tests are similar to multiple testing correction. The difference is that for repeated testing we know that the tests are correlated (as we are measuring repeatedly on the same increasing sample) and can therefore do more efficient corrections, which is what sequential tests are doing.\n",
    "\n",
    "\n",
    "Below is a simulation with repeated testing and no correction, Bonferroni, and always valid sequantial tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1260de0a-e2fb-4987-8343-7835f112e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci(alpha, ro, n):\n",
    "    # This is an always valid CI from https://arxiv.org/pdf/2302.10108.pdf?fbclid=IwAR11HKKqFiMUinXj2NrvpOA0KQPAJPpbRTS0SPMQj5W82gsqctnwESw0hHY\n",
    "    return math.sqrt(2 * (n * ro**2 + 1)/(n**2 * ro**2) * math.log(math.sqrt(n * ro**2 + 1) / alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e83bde-23c1-4ba1-84df-f8d3bdeffcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate under repeated testing with standard inference: 0.198\n",
      "False positive rate under repeated testing with Bonferroni correction: 0.032\n",
      "False positive rate under repeated testing with always valid inference: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Check false positive rate over repeated testing with different methods\n",
    "sig_no_corr = []\n",
    "sig_bonferroni = []\n",
    "sig_corr = []\n",
    "n_samples = 1000\n",
    "alpha = 0.05\n",
    "T = 10 # Number of repeated tests per sample\n",
    "ro = 1\n",
    "p = 0.5\n",
    "for j in range(500):\n",
    "    result_df = pd.DataFrame({'metric': np.random.normal(10, 2.5, n_samples*2), 'group': np.random.binomial(1, 0.5, n_samples*2)})\n",
    "    X = sm.add_constant(result_df[['group']])\n",
    "    y = result_df['metric']\n",
    "    Z = ((result_df['group']/p) - ((1 - result_df['group'])/(1 - p))) * y\n",
    "    sig_temp_no_corr = []\n",
    "    sig_temp_bonferroni = []\n",
    "    sig_temp_corr = []\n",
    "    for i in range(T):\n",
    "        curr_n = int(n_samples*2*((i+1)/T))\n",
    "\n",
    "        # with standard inference\n",
    "        model = sm.OLS(y.head(curr_n), X.head(curr_n)).fit()\n",
    "        sig_temp_no_corr.append(model.summary2().tables[1]['P>|t|'].iloc[1]<=alpha)\n",
    "\n",
    "        # with standard inference and Bonferroni\n",
    "        sig_temp_bonferroni.append(model.summary2().tables[1]['P>|t|'].iloc[1]<=alpha/T)\n",
    "        \n",
    "        # with always valid inference\n",
    "        sig_temp_corr.append(Z.head(curr_n).mean() - math.sqrt(np.var(Z.head(curr_n))) * ci(alpha, ro, curr_n) >= 0 or Z.head(curr_n).mean() + math.sqrt(np.var(Z.head(curr_n))) * ci(alpha, ro, curr_n) <= 0)  \n",
    "\n",
    "    sig_no_corr.append(any(sig_temp_no_corr))\n",
    "    sig_bonferroni.append(any(sig_temp_bonferroni))\n",
    "    sig_corr.append(any(sig_temp_corr))\n",
    "    \n",
    "print(f'False positive rate under repeated testing with standard inference: {np.mean(sig_no_corr)}')\n",
    "print(f'False positive rate under repeated testing with Bonferroni correction: {np.mean(sig_bonferroni)}')\n",
    "print(f'False positive rate under repeated testing with always valid inference: {np.mean(sig_corr)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad871e9-7b87-4dbf-a721-7cf3a00136cd",
   "metadata": {},
   "source": [
    "We can see that with standard inference the false positive rate is very inflated compared to the intented alpha of 0.05. Bonferroni is bounding the false positive rate as expected, it is conservative because it does not take the correlation into account. With always valid inference the false positive rate is far below alpha. This is expected since the always valid test corrects for looking after each user coming in and we only look 10 times for 2000 users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b1be7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# False negative rate and sample size calculation\n",
    "The intended false negative rate, often denoted beta (which is 1 minus the intended power), is the rate at which we don't detect a certain treatment effect when it in fact exists. If we design our experiment such that we meet the required sample size to achieve the power, and we fulfill all other assumptions of the test, we obtain the intended false negative rate (or lower) over repeated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5decdce-2282-47a3-a166-bb47df73f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140\n"
     ]
    }
   ],
   "source": [
    "# Using a package\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "ratio = 1\n",
    "effect_size = 0.1\n",
    "N1 = sms.NormalIndPower().solve_power(effect_size=effect_size, nobs1=None, alpha=alpha, power=1-beta, ratio=ratio, alternative='two-sided')\n",
    "N2 = N1*ratio\n",
    "print(round(N1+N2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4a426f-cf10-4fc9-933f-902a0a17176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation\n",
    "def required_sample_size_z_test(\n",
    "    alpha=0.05, beta=0.2, variance=1, abs_MDE=0.1, treat_p=0.5\n",
    "):\n",
    "    kappa = treat_p / (1 - treat_p)\n",
    "    nB = (\n",
    "        (norm.ppf(1 - alpha) + norm.ppf(1 - beta)) ** 2\n",
    "        / abs_MDE**2\n",
    "        * variance\n",
    "        * (1 + 1 / kappa)\n",
    "    )\n",
    "    N = nB * (1 + kappa)\n",
    "    return int(np.ceil(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1d1b8e-f6a3-4909-9743-021751a178c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140\n"
     ]
    }
   ],
   "source": [
    "manual_ss = required_sample_size_z_test(alpha=alpha/2, beta=beta, variance=1, abs_MDE=0.1, treat_p=0.5)\n",
    "print(manual_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049d94e1-4d2e-4d03-b5d2-6dc870be81c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check package and I agree\n",
    "manual_ss == round(N1+N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593dabd-3d0e-40cb-ba53-05fbd428d0c6",
   "metadata": {},
   "source": [
    "Below is a simulation were we have powered the experiment to 80% for the true treatment effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701f2ee0-3ee3-4ed9-a585-a205c002ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power is 81.2%.\n"
     ]
    }
   ],
   "source": [
    "sigZ = []\n",
    "\n",
    "reps = 1000\n",
    "effect = 0.1\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "n = manual_ss//2\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(reps):\n",
    "    y0 = np.random.normal(mean, std_dev, n)\n",
    "    y1 = np.random.normal(mean, std_dev, n) + effect\n",
    "    \n",
    "    stat, pvalue = ztest(x1=y1, x2=y0, alternative='two-sided')\n",
    "    sigZ.append(pvalue<=alpha)\n",
    "    \n",
    "print(f\"The power is {round(np.mean(sigZ)*100,2)}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadd2e2-2ead-4de0-94fd-ba85b0dfac54",
   "metadata": {},
   "source": [
    "If indeed we use the required sample size returned by the sample size caluclation, and we generate data with the hypothesised effect, we obtain the intented power over repeated experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ecd90-b28e-4707-a58a-5c128967f486",
   "metadata": {},
   "source": [
    "# Experiment efficiency and Experimental design\n",
    "There are essentially two steps to any experimental design. The sampling, i.e., which subset of the population that is in our sample, and, the treatment assignment, i.e., which users in our sample that is in treatment and control, respectively (sticking to two groups here for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef53acc",
   "metadata": {},
   "source": [
    "## Sampling design\n",
    "Sampling design is about making the sample as representative of the population as possible. There are two main reasons for not just drawing a random sample from the population:\n",
    "1. There is a risk that by chance certain types of users are not represented in the sample (inclusion).\n",
    "2. We can reduce the variance in our estimators over repeated sampling by ensuring that our sample is a good representation of the population. \n",
    "\n",
    "Sampling design can be done before or after the experiment. The most common pre-experiment sampling design is stratified sampling, where the population is split into important substrata and we are taking random samples from each strata (proportional to their relative size) to ensure all strata are represented in the sample. It is also possible to do sampling design after the experiment. The most common method is called post stratification. In post-stratification we are using our knowledge about the true proportion of the strata in the population to re-weight the treatment effect estimator. For example, is there is 50/50 young/old in the population but we have 70/30 in our sample we would down-weight the young users and up-weight the old users in our estimator to obtain an unbiased population average treatment effect estimator, even though our sampling is biased. \n",
    "\n",
    "### Practical concerns\n",
    "In online experimentation, we can typically not control sampling. That is, users come in whenever they are active and we are usually not willing to wait for certain users for too long. Moreover, stratified sampling is somewhat complicated (at least costly) to achieve technically in a feature-flagging system. For this reason post-stratification is the most plausible alternative. Post stratification can of course not solve the issue of some users not being represented at all in the sample, however, in large samples it is very unlikley that groups that are not too small in the population are not at all represented in the sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31613e4",
   "metadata": {},
   "source": [
    "## Treatment assignment design\n",
    "Treatment assignment design is all about restricting the random treatment assignment to not allow treatment assignments that we know are not giving similar groups in terms of the outcome. Of course we do not know the outcome at the design stage, but if we have covariates that are correlated with the outcome, we know that imbalance in the covariates is likley to imply imbalance in the outcome.  \n",
    "\n",
    "Similar to sampling design, there are two main reasons for restricting the randomization with a design: \n",
    " 1. There is a risk that by chance certain types of users are not represented in all treatment groups (inclusion).\n",
    " 2. We can reduce the variance in our estimators over repeated experimenting by ensuring that our treatment groups are as similar as possible in terms of covariates that are correlated with the outcome.\n",
    "\n",
    "\n",
    "\n",
    " Treatment assignment design can be done before and/or after the experiment. The most common pre-experiment treatment assignment designs are stratified randomization and rerandomization. The most common post-experiment design is regression adjustment (also called cuped). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47d081",
   "metadata": {},
   "source": [
    "### Rerandomization\n",
    "Stratified randomization is a special case of rerandomization, so I stick to rerandomization here to exemplify what I know about design. \n",
    "Rerandomization is simply re-randomizing the treatment assignment until the groups are similar enough on some similarity measure. Since this decreases the variance of the estimator, the statistical inference must be adjusted accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdb586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reps = 1000\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "n = 2000\n",
    "alpha = 0.05\n",
    "rerand_n = 20\n",
    "\n",
    "std_err_ratio = []\n",
    "sigZ = []\n",
    "sigOLS = []\n",
    "sigRR_Z = []\n",
    "sigRR_OLS = []\n",
    "estimates_no_RR = []\n",
    "estimates_no_RR_OLS = [] \n",
    "estimates_RR = []\n",
    "\n",
    "for i in range(reps):\n",
    "    # generating data\n",
    "    x = np.random.normal(mean, std_dev, n*2)\n",
    "    y = x + np.random.normal(mean, std_dev, n*2)\n",
    "    treat = np.random.binomial(1,0.5,n*2) \n",
    "\n",
    "    # Without rerandomization\n",
    "    ## standard z-test\n",
    "    stat, pvalue = ztest(x1=y[treat==1], x2=y[treat==0], alternative='two-sided')\n",
    "    sigZ.append(pvalue<=alpha)\n",
    "    estimates_no_RR.append(np.mean(y[treat==1]) - np.mean(y[treat==0]))\n",
    "\n",
    "    # using OLS (standard variance reduction)\n",
    "    X = np.column_stack((x,treat))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    sigOLS.append(model.pvalues[2]<=alpha)\n",
    "    estimates_no_RR_OLS.append(model.params[2])\n",
    "\n",
    "    # rerandomize\n",
    "    diff = float('inf')\n",
    "    for r in range(rerand_n):\n",
    "        treat_temp = np.random.binomial(1,0.5,n*2) \n",
    "        diff_temp = (np.mean(x[treat_temp==1])-np.mean(x[treat_temp==0]))**2\n",
    "        if diff_temp < diff:\n",
    "            treat = treat_temp\n",
    "            diff = diff_temp\n",
    "        \n",
    "    # inference after rerandomization\n",
    "\n",
    "    ## Not taking rerandomization into account\n",
    "    stat, pvalue = ztest(x1=y[treat==1], x2=y[treat==0], alternative='two-sided')\n",
    "    sigRR_Z.append(pvalue<=alpha)\n",
    "    \n",
    "    ## Taking rerandomization into account\n",
    "    X = np.column_stack((x,treat))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    sigRR_OLS.append(model.pvalues[2]<=alpha)\n",
    "    estimates_RR.append(model.params[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be805236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The false positive rate for z test without rerandomization:0.055\n",
      "The false positive rate for z test with rerandomization 0.003\n",
      "The false positive rate for OLS-z-test without rerandomization 0.048\n",
      "The false positive rate for OLS-z-test with rerandomization 0.045\n"
     ]
    }
   ],
   "source": [
    "print(f\"The false positive rate for z test without rerandomization:{np.mean(sigZ)}\")\n",
    "print(f\"The false positive rate for z test with rerandomization {np.mean(sigRR_Z)}\")\n",
    "print(f\"The false positive rate for OLS-z-test without rerandomization {np.mean(sigOLS)}\")\n",
    "print(f\"The false positive rate for OLS-z-test with rerandomization {np.mean(sigRR_OLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09663fde-5273-49c4-b6e2-2cbed4c1382a",
   "metadata": {},
   "source": [
    "As we have seen before, using a z test without rerandomization works as intended. Using rerandomization in combinaition with a standard z test (that doesn't take the rerandomization into account) gives a very conservative test, as the rerandomization removes some of the variation. One valid inference approach after rerandomization is to use OLS with the same covariate as in the rerandomization. As we can see rerandomization combined with OLS give the right false positive rate.\n",
    "\n",
    "Of course, an alternative used by many online experimenters is variance reduction, that is only OLS with the covariate after the experiment. As expected this too gives the right false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0301a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of the estimator with rerandomization is 52.54 % smaller than without\n",
      "The variance of the estimator with only post-variance reduction is 51.92 % smaller than without\n"
     ]
    }
   ],
   "source": [
    "print(f\"The variance of the estimator with rerandomization is {np.round((np.var(estimates_no_RR) - np.var(estimates_RR))/ np.var(estimates_no_RR) * 100,2 ) } % smaller than without\" )\n",
    "print(f\"The variance of the estimator with only post-variance reduction is {np.round((np.var(estimates_no_RR) - np.var(estimates_no_RR_OLS))/ np.var(estimates_no_RR) * 100,2 ) } % smaller than without\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774cc9ae-ad09-433b-9800-5faba7a307bc",
   "metadata": {},
   "source": [
    "In terms of variance reduction, reandomization in combination with OLS on one hand, and only using OLS on the other, are comparable. In large sample, the reduction will be the same. This indicated that for large sample sizes the only reason for doing it at the design stage is the inclusion argument discussed above.\n",
    "\n",
    "Source: https://academic.oup.com/jrsssb/article/82/1/241/7056044?fbclid=IwAR1br3pbcE0XrJOmQF8T-PJAPQmnSUvCPazw_Nlxed6RMODbyN-dLquYrLM&login=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604b8b2-ace3-4ad1-bda0-accf077cfd7e",
   "metadata": {},
   "source": [
    "# Other tests\n",
    "In online A/B testing almost all tests are mean based which means that we can use z and t test in large samples. There are some exceptions. \n",
    "\n",
    "## Chi-square\n",
    "One test that almost all experientation tools provide is the chi square test, which in this context is used for sample ratio mismatch, which is an experiment validity test.\n",
    "\n",
    "Below is simulation testing two proportions over repeated randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81574f85-a993-4ae9-ba4e-46cc688dedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046\n",
      "0.046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigChiSquare = []\n",
    "sigManualChiSquare = []\n",
    "\n",
    "reps = 1000\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "n = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "for i in range(reps):\n",
    "    treat = np.random.binomial(1, 0.5, n*2)\n",
    "    \n",
    "    # Using a package\n",
    "    res = chisquare([np.sum(treat==1),np.sum(treat==0)], f_exp=[n,n])\n",
    "    sigChiSquare.append(res.pvalue<=alpha)\n",
    "\n",
    "    # Manually\n",
    "    chisqr = (np.sum(treat==1)-n)**2/n + (np.sum(treat==0)-n)**2/n\n",
    "    pvalue = 1 - chi2.cdf(chisqr, 1)\n",
    "    sigManualChiSquare.append(pvalue<=alpha)\n",
    "    \n",
    "print(np.mean(sigChiSquare))\n",
    "print(np.mean(sigManualChiSquare))\n",
    "sigChiSquare == sigManualChiSquare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bc7eb-4242-4202-8429-ca4b8ba92496",
   "metadata": {},
   "source": [
    "As we can see the test has the intended false positive rate.\n",
    "## Bootstrap\n",
    "An inference method that is useful for estimators where the sampling distribution is hard to obtain is bootstrap. Bootstrap is a resampling method for appoximating the sampling distributuon of any estimator. Below I have implemented it for the difference-in-medians estimator. Bootstrap is computationaly complex but there are methods to make it more efficient for example poisson bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a618a8c-0ea9-4354-881b-9637cddb36b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coverage of the bootstrap CI for the diffeence-in-medians is 89.1%\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_median_diff_ci(y, treat,alpha, b):\n",
    "    y1 = y[treat==1]\n",
    "    n1 = np.sum(treat==1) \n",
    "    y0 = y[treat==0]\n",
    "    n0 = np.sum(treat==0)\n",
    "    diff = []\n",
    "    for i in range(b):\n",
    "        diff.append(np.median(np.random.choice(y1, n1, replace=True))-np.median(np.random.choice(y0, n0, replace=True)))\n",
    "\n",
    "    return np.quantile(diff, [alpha/2, 1-alpha/2])\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "reps = 1000\n",
    "n_samples = 100\n",
    "sig = []\n",
    "for i in range(reps):\n",
    "    temp_ci = bootstrap_median_diff_ci(np.random.normal(1,1,n_samples), np.random.binomial(1,0.5, n_samples),alpha = alpha, b=100)\n",
    "    sig.append(temp_ci[0] > 0 or temp_ci[1]<0)\n",
    "    \n",
    "print(f\"The coverage of the bootstrap CI for the diffeence-in-medians is {(1-np.mean(sig)) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031168b-49d5-416a-9e07-9c642a4367f6",
   "metadata": {},
   "source": [
    "As we can see the coverage is close to the intended 90% already with 100 bootstrap samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
